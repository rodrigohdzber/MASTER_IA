{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4 elimina split y contrasplit en RV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ya tenemos los datos descargados, homogeneizados y limpios. Sin embargo es probable que tengamos que ajustar la serie. \n",
    "- Debemos buscar los split y contrasplit, calcular su efecto y ajustar la serie temporal, tantas veces como hayan sucedido.\n",
    "- Una empresa puede valer 100€ teniendo 10 acciones de 10€ o teniendo 100 acciones de 1€. El precio de cotización varía drásticamente, pero el valor de la compañía no.\n",
    "- Por desgracia, con los datos que nos hemos bajado, no disponemos ni del número de acciones, ni conocemos cuando han sucedido los split o contrasplit.\n",
    "- Por otro lado, dentro de los datos que nos bajamos, sí que tenemos el cierre ajustado por dividendos; por lo que este problema nos lo dan resuelto.\n",
    "- Deberemos trabajar con ese cierre ajustado.\n",
    "\n",
    "- Objetivos:\n",
    "    - Encuentra y anula los split y contrasplit\n",
    "    - Programa una función a la que pasar un índice y que guarde en excel los datos del índice y de sus activos (homogeneizados, limpios y ajustados).\n",
    "- Es decir 6 excels: Uno para el índice y 5 (High, Low, Volume, Adj_close y Open) para los activos, donde se agrupan los datos de todos los activos del índice.\n",
    "\n",
    "Tiempo objetivo: 45 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from time import mktime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from rcurl import get_curl\n",
    "from io import BytesIO\n",
    "import pandas.io.formats.excel\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_crumbs_and_cookies(stock):\n",
    "    \"\"\"\n",
    "    get crumb and cookies for historical data csv download from yahoo finance  \n",
    "    parameters: stock - short-handle identifier of the company    \n",
    "    returns a tuple of header, crumb and cookie\n",
    "    \"\"\"   \n",
    "    url = 'https://finance.yahoo.com/quote/{}/history'.format(stock)\n",
    "    \n",
    "    with requests.session():\n",
    "        header = {'Connection': 'keep-alive',\n",
    "                   'Expires': '-1',\n",
    "                   'Upgrade-Insecure-Requests': '1',\n",
    "                   'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) \\\n",
    "                   AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36'\n",
    "                   }        \n",
    "        website = requests.get(url, headers=header)\n",
    "        soup = BeautifulSoup(website.text, 'lxml')\n",
    "        \n",
    "        crumb = re.findall('\"CrumbStore\":{\"crumb\":\"(.+?)\"}', str(soup))\n",
    "        output=(header, crumb[0], website.cookies)\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unix(date):\n",
    "    \"\"\"\n",
    "    converts date to unix timestamp    \n",
    "    parameters: date - in format (yyyy-mm-dd)    \n",
    "    returns integer unix timestamp\n",
    "    \"\"\"\n",
    "    datum = datetime.strptime(date, '%Y-%m-%d')\n",
    "    \n",
    "    return int(mktime(datum.timetuple()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert_to_unix('2020-04-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(stock, interval='1d', day_begin='20-03-2018', day_end='20-06-2018'):\n",
    "    \"\"\"\n",
    "    queries yahoo finance api to receive historical data in csv file format\n",
    "    \n",
    "    parameters: \n",
    "        stock - short-handle identifier of the company        \n",
    "        interval - 1d, 1wk, 1mo - daily, weekly monthly data        \n",
    "        day_begin - starting date for the historical data (format: dd-mm-yyyy)        \n",
    "        day_end - final date of the data (format: dd-mm-yyyy)\n",
    "    \n",
    "    returns a list of comma seperated value lines\n",
    "    \"\"\"\n",
    "    \n",
    "    error1='404 Not Found: Timestamp data missing.'\n",
    "    \n",
    "    day_begin_unix = convert_to_unix(day_begin)\n",
    "    day_end_unix = convert_to_unix(day_end)   \n",
    "    \n",
    "    header, crumb, cookies = _get_crumbs_and_cookies(stock)\n",
    "    \n",
    "    with requests.session():\n",
    "        \n",
    "        url = 'https://query1.finance.yahoo.com/v7/finance/download/' \\\n",
    "              '{stock}?period1={day_begin}&period2={day_end}&interval={interval}&events=history&crumb={crumb}' \\\n",
    "              .format(stock=stock, day_begin=day_begin_unix, day_end=day_end_unix, interval=interval, crumb=crumb)\n",
    "                \n",
    "        website = requests.get(url, headers=header, cookies=cookies)\n",
    "        \n",
    "        return website.text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dates(x,ventana):\n",
    "    \n",
    "    \"\"\"setup inicial de fechas,\n",
    "    esta funcion devuelve la fecha de inicio\n",
    "    y fin en string y formato YYYY-MM-DD\n",
    "    Parametros:\n",
    "    x=start o end\n",
    "    ventana: entero referido al numero de dias del periodo\"\"\"\n",
    "    \n",
    "    hoy=datetime.now()\n",
    "    if x=='end':\n",
    "        fecha_fin = str(hoy.now())\n",
    "        fecha_fin = fecha_fin[0:10]\n",
    "        return (fecha_fin)\n",
    "    if x=='start':\n",
    "        fecha_inicial=hoy-timedelta(ventana)\n",
    "        fecha_inicial = str(fecha_inicial)\n",
    "        fecha_inicial = fecha_inicial[0:10]\n",
    "        return (fecha_inicial)\n",
    "    else:\n",
    "        print('inputs incorrectos. Ver docstring d ela funcion')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intentos_descarga_precios(stock_series,interval,fecha_inicial,fecha_fin, index_series, intentos):\n",
    "    \"\"\"funcion que en funcion del resultado de la descarga,\n",
    "    devuelve un df con la informacion, y un codigo 0 'OK'\n",
    "    o un codigo 1 de error y un DF vacio\"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = historic_prices_series(stock_series,interval,fecha_inicial,fecha_fin, index_series)\n",
    "        codigo = 'OK'\n",
    "        e = 'OK'\n",
    "        return df, codigo, e\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        print('El activo no tiene activos descargables')\n",
    "        print('Mensaje de error: ',str(e))\n",
    "        print('Intento numero ',str(intentos))\n",
    "        time.sleep(3)\n",
    "        df = pd.DataFrame ({'dato':[],\n",
    "                            'Divisa':[]})\n",
    "        codigo = 'ERROR'\n",
    "        return df, codigo, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historic_prices_series(stock_series,interval,fecha_inicial,fecha_fin, index_series):\n",
    "    n=0\n",
    "    x=0 \n",
    "    for i in range(len(stock_series)):\n",
    "        print(\"descargando \"+stock_series[i]+' del indice '+index_series[i])\n",
    "        try:\n",
    "            DF=load_csv_data(stock_series[i], interval=interval, day_begin=fecha_inicial, day_end=fecha_fin)\n",
    "            DF=pd.DataFrame(DF)\n",
    "            DF = DF.iloc[:,0].str.split(\",\", n = 7, expand = True)\n",
    "            DF.columns=DF.iloc[0,:]\n",
    "            DF=DF.iloc[1:DF.shape[0],:]\n",
    "            x=DF.shape[1]                 \n",
    "            if x<2:\n",
    "                 print(\"No nos hemos podido descargar el activo\", benchmark)\n",
    "            else:\n",
    "                DF['stock'] = str(stock_series[i])\n",
    "                DF['indice'] = str(index_series[i]) \n",
    "                DF=DF.dropna()\n",
    "            \n",
    "                if n==0:\n",
    "                    dfacum=DF\n",
    "                else:\n",
    "                    dfacum=agrega_dataframes(DF,dfacum)    \n",
    "                n=n+1\n",
    "        except Exception as e:\n",
    "            print(\"No nos hemos podido descargar el activo\")\n",
    "            print('key error:', e)\n",
    "            next\n",
    "        \n",
    "    return dfacum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrega_dataframes(df,dfacum):\n",
    "    \"\"\"codigo que permite ir concatenando\n",
    "    dataframes con estructura similar\n",
    "    df:daframe a agregar\n",
    "    dfacum: dataframe en donde se agregara\"\"\"\n",
    "    \n",
    "    dfacum=pd.concat([dfacum, df], axis=0,sort=True)\n",
    "    dfacum.reset_index(drop=True)\n",
    "    return dfacum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_acum_by_stock(dfacum):\n",
    "    \"\"\"codigo que permite ir desconcatenando\n",
    "    dataframes con estructura similar a partir de un df acumulado\n",
    "    requiere que la clave de desagregacion sea la columna 'stock'\n",
    "    dfacum: dataframe en se ha agregado\"\"\"\n",
    "    \n",
    "    activos_a_importar = dfacum.stock.unique()\n",
    "    for activo in activos_a_importar:\n",
    "        globals()[activo] = dfacum[dfacum['stock']==activo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_lab_dates(fecha_inicial,ventana):\n",
    "    \"\"\"con esta funcion se obtiene un listado de fechas\n",
    "    entre lunes y viernes, a partir de una fecha inicial\n",
    "    y una ventana\"\"\"\n",
    "    \n",
    "    dates = pd.date_range(fecha_inicial, periods=ventana, freq='B')    \n",
    "    inicio = datetime.strptime(fecha_inicial, '%Y-%m-%d')\n",
    "    dias =ventana\n",
    "    dates=[]\n",
    "    \n",
    "    for days in range(dias):\n",
    "        date = inicio + timedelta(days=days)\n",
    "        if date.weekday() < 5:\n",
    "            dates.append(date)\n",
    "            \n",
    "    return pd.DataFrame({'dates':dates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_df_full(ventana, end, start):\n",
    "    \n",
    "    fecha_fin=set_dates('end',ventana)\n",
    "    fecha_inicial=set_dates('start',ventana)\n",
    "    print('la fecha de inicio es',fecha_inicial,' y la de fin es',fecha_fin)\n",
    "    dates_df = vector_lab_dates(fecha_inicial, ventana)\n",
    "    \n",
    "    return dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def componentes_indice_ind_download_0(ticker):\n",
    "    \"\"\"esta funcion devuelve los componentes y divisa de cada indice.\n",
    "    como parametro se debe inclur los tickers de Yahoo Finance de los indices\n",
    "    en una tupla\"\"\"\n",
    "    \n",
    "    lista_dato = []\n",
    "    \n",
    "    #primero se obtiene la divisa\n",
    "    url=\"https://es.finance.yahoo.com/quote/\"+ticker+\"/components/\"\n",
    "    soup  = requests.get(url)\n",
    "    soup  = BeautifulSoup(soup.content, 'html.parser')\n",
    "    \n",
    "    name_box = soup.find('span', attrs={'data-reactid': '4'})\n",
    "    name = name_box.text.strip()\n",
    "    divisa=name[len(name)-3:len(name)]\n",
    "    \n",
    "    print(ticker,divisa)\n",
    "    soup  = requests.get(url)\n",
    "    soup  = BeautifulSoup(soup.content, 'html.parser')\n",
    "    \n",
    "    name_box = soup.find_all('td')\n",
    "    name_boxccc=name_box[0]\n",
    "    name_empresa=name_box[1]\n",
    "    name_boxccc = name_boxccc.get_text(strip=True)\n",
    "    name_empresa = name_empresa.get_text(strip=True)\n",
    "    \n",
    "    for i in range(0,len(name_box)):\n",
    "        name_boxccc=name_box[i]\n",
    "        name_boxccc = name_boxccc.get_text(strip=True)\n",
    "        lista_dato.append(name_boxccc)\n",
    "    df=pd.DataFrame ({'dato':lista_dato})\n",
    "    df['Divisa'] = divisa\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intentos_descarga_indice(ticker, intentos):\n",
    "    \"\"\"funcion que en funcion del resultado de la descarga,\n",
    "    devuelve un df con la informacion, y un codigo 0 'OK'\n",
    "    o un codigo 1 de error y un DF vacio\"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = componentes_indice_ind_download_0(ticker)\n",
    "        codigo = 'OK'\n",
    "        e = 'OK'\n",
    "        print('El índice ', ticker,' se ha descargado correctamente')\n",
    "        return df, codigo, e\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('El índice ', ticker,' no tiene activos descargables')\n",
    "        print('Mensaje de error: ',str(e))\n",
    "        print('Intento numero ',str(intentos))\n",
    "        time.sleep(3)\n",
    "        df = pd.DataFrame ({'dato':[],\n",
    "                            'Divisa':[]})\n",
    "        codigo = 'ERROR'\n",
    "        return df, codigo, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def componentes_indice_ind_download_1(ticker):\n",
    "    \"\"\"esta funcion devuelve los componentes y divisa de cada indice.\n",
    "    como parametro se debe inclur los tickers de Yahoo Finance de los indices\n",
    "    en una tupla\"\"\"\n",
    "    \n",
    "    intentos = 1\n",
    "    for i in range(3):\n",
    "        \n",
    "        intentos_descarga_indice_res = intentos_descarga_indice(ticker, intentos)\n",
    "        df = intentos_descarga_indice_res[0]\n",
    "        codigo = intentos_descarga_indice_res[1]\n",
    "        e = intentos_descarga_indice_res[2]\n",
    "        \n",
    "        if codigo == 'OK':\n",
    "            return df\n",
    "        \n",
    "        if (codigo == 'ERROR') & (intentos <=3):\n",
    "            intentos = intentos + 1\n",
    "            time.sleep(3)\n",
    "            \n",
    "        if intentos ==3:\n",
    "            print('Mensaje de error: ',str(e))\n",
    "            print(\"Tras varios intentos no nos hemos podido descargar\",ticker,\"Dejamos de intentarlo.\")\n",
    "            df = pd.DataFrame ({'dato':[],\n",
    "                                'Divisa':[]})\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupera_valor_secuencia(df, start, freq, encabezado):\n",
    "    valor = pd.DataFrame(np.arange(start, len(df), freq))\n",
    "    valor['key'] = 1\n",
    "    valor.index = valor[0]\n",
    "    valor = valor.drop(0, 1)\n",
    "    valor = pd.merge(df, valor, left_index=True, right_index=True)\n",
    "    valor.reset_index(drop=True, inplace=True)\n",
    "    valor.rename(columns={\"dato\": encabezado},inplace=True)\n",
    "    valor = valor.drop(['key'], 1)\n",
    "    return valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def componentes_indice_ind_download_2(tickers_de_indices):\n",
    "    \"\"\"En este tercer paso, con la informacion descargada,reconstruimos la tabla. \n",
    "    Tiene 6 columnas (simbolo, nombre de la empresa, último precio, cambio, cambio % y volumen.)\n",
    "    # Nos interesa obtener el símbolo y el nombre de la empresa.\n",
    "    # Ojo, queremos importar únicamente los activos que tengan cotización. \n",
    "    \"\"\"\n",
    "    intentos = 0\n",
    "    \n",
    "    dfacum = pd.DataFrame ({'Activo':[],\n",
    "                            'Divisa':[],\n",
    "                            'Empresa':[],\n",
    "                            'Precio':[],\n",
    "                            'Indice':[]})\n",
    "    \n",
    "    for indice in tickers_de_indices:\n",
    "        df = componentes_indice_ind_download_1(indice)\n",
    "        Activo = recupera_valor_secuencia(df, 0, 6, 'Activo')\n",
    "        df = df.drop(['Divisa'],1)\n",
    "        Empresa = recupera_valor_secuencia(df, 1, 6, 'Empresa')\n",
    "        Precio = recupera_valor_secuencia(df, 2, 6, 'Precio')\n",
    "        df = pd.merge(Activo, Empresa, left_index=True, right_index=True)\n",
    "        df = pd.merge(df, Precio, left_index=True, right_index=True)\n",
    "        df['Indice'] = indice\n",
    "        df = df[df.loc[:,'Precio'] != '']\n",
    "        dfacum = agrega_dataframes(dfacum, df)\n",
    "        dfacum.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    return dfacum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depura_df_precios(dfacum):\n",
    "    dfacum=dfacum.loc[:,['Adj Close','Close', 'Date','High','Low','Open','Volume','indice','stock']]\n",
    "    (dfacum[(dfacum.Date).isna()]) = (dfacum[(dfacum.Date).isna()]).dropna()\n",
    "    return dfacum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_date_adj(dfacum, dates_df):\n",
    "    \"\"\"ajustamos por  fechas.\n",
    "    las pasamos como indice y armonizamos fechas.\n",
    "    dates_df han pasado por una funcion en donde se han depurado sabados y domingos\n",
    "    dfacum: dataframe con los campos, Adj Close, Close, Date, High, Low, Open, Volume, indice,stock \n",
    "    dates_df: : dataframe con las fechas que seran la muestra final\"\"\"\n",
    "    \n",
    "    dfacum.index=dfacum.Date\n",
    "    dates_df.index=dates_df.dates\n",
    "    dates_df.index = pd.to_datetime(dates_df.index).strftime('%d-%m-%Y')\n",
    "    dfacum.index = pd.to_datetime(dfacum.index).strftime('%d-%m-%Y')                            \n",
    "    df_date_adj=pd.merge(dates_df, dfacum,left_index=True, right_index=True)\n",
    "    df_date_adj = df_date_adj.drop(['dates'],1)\n",
    "    df_date_adj = df_date_adj.drop(['Date'],1)\n",
    "    \n",
    "    df_date_adj.sort_index(axis = 0)\n",
    "    return df_date_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remplaza_nulos_na(df, registros_a_depurar):\n",
    "    \"\"\"Con esta funcion se rellenan datos no informados.\n",
    "    En un primer paso, los nulos por nas, y en un segundo paso,\n",
    "    los na por el dato de la fecha previa, y para los primeros daros de la \n",
    "    df: dataframe con los campos, Adj Close, Close, Date, High, Low, Open, Volume, indice,stock \n",
    "    activos: dataframe con los nombres de los activos a los que se aplica\"\"\"\n",
    "    \n",
    "    #Se identifican los los nulos y se reemplazan con NA\n",
    "    df[df == 'null'] = np.nan\n",
    "    df.replace('', np.nan)\n",
    "    df.replace(' ', np.nan)\n",
    "    df.replace('-', np.nan)\n",
    "    df.replace('NA', np.nan)\n",
    "    \n",
    "    for activo in registros_a_depurar:\n",
    "        df[activo] = df[activo].fillna(method='pad')\n",
    "        df[activo] = df[activo].fillna(method='bfill')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_de_datos(dfacum, dates_df):\n",
    "    \"\"\"funcion que agrega las rutinas definidas en el apartado para la depuracion de datos, \n",
    "    en base a fechas, y gestion de ausencias.\n",
    "    como inputs el dataframe global y el dataframe de fechas laborables\"\"\"\n",
    "    \n",
    "    activos_a_importar = dfacum.stock.unique()\n",
    "    \n",
    "    for activo in activos_a_importar:\n",
    "        globals()[activo] = dataframe_date_adj(globals()[activo], dates_df)\n",
    "        globals()[activo] = remplaza_nulos_na(globals()[activo], globals()[activo].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_OCHLV_DF(dfacum, dates_df):\n",
    "    \n",
    "    activos_a_importar = dfacum.stock.unique()\n",
    "    fechas=len(dates_df.index)\n",
    "    nactivos=len(activos_a_importar)\n",
    "    matriz=np.zeros((fechas, nactivos))\n",
    "    carcasa=pd.DataFrame(matriz, index=dates_df.index, columns=activos_a_importar)\n",
    "    Open = carcasa.copy()\n",
    "    High = carcasa.copy()\n",
    "    Low = carcasa.copy()\n",
    "    Volume = carcasa.copy()\n",
    "    Adj_Close = carcasa.copy()\n",
    "    \n",
    "    for activo in activos_a_importar:\n",
    "        datos = globals().get(activo)\n",
    "        Open[activo] = datos.loc[(datos['stock']==activo) == True , 'Open']\n",
    "        High[activo] = datos.loc[(datos['stock']==activo) == True , 'High']\n",
    "        Low[activo] = datos.loc[(datos['stock']==activo) == True , 'Low']\n",
    "        Volume[activo] = datos.loc[(datos['stock']==activo) == True , 'Volume']\n",
    "        Adj_Close[activo] = datos.loc[(datos['stock']==activo) == True , 'Adj Close']\n",
    "        \n",
    "    Open = remplaza_nulos_na(Open, Open.columns)\n",
    "    High = remplaza_nulos_na(High, High.columns)\n",
    "    Low = remplaza_nulos_na(Low, Low.columns)\n",
    "    Volume = remplaza_nulos_na(Volume, Volume.columns)\n",
    "    Adj_Close = remplaza_nulos_na(Adj_Close, Adj_Close.columns)\n",
    "    \n",
    "    return Open, High, Low, Adj_Close, Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rentabilidades_diarias(df):\n",
    "    \n",
    "    # Los datos de cierre ajustados no están bien desmanipulados.\n",
    "    # calculamos la rentabilidad de los activos\n",
    "    \n",
    "    rentabilidades = pd.DataFrame(0,\n",
    "                                  index=df.index,\n",
    "                                  columns=df.columns)\n",
    "    \n",
    "    # Ojo, los activos que no nos hemos podido descargar tienen precio de 0 €. La rentabilidad 0/0 dará un error que tenemos que evitar.\n",
    "    rentabilidades = np.log(df.astype(float)).diff()\n",
    "    rentabilidades = rentabilidades.dropna() #eliminamos la tipca primera fila de NaN\n",
    "    \n",
    "    return rentabilidades\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajusta_serie_split(rentabilidades, Adj_Close, High, Low, Open):\n",
    "    for activo in rentabilidades.columns:\n",
    "\n",
    "    # Comprobamos si alguna rentabilidad de la serie es mayor o menor a 0.69 y calculamos los multiplicadores.\n",
    "        indice_booleano = (rentabilidades[[activo]] > 0.69) | (rentabilidades[[activo]] < -0.69)\n",
    "        rentabilidades_excesivas = rentabilidades[[activo]][indice_booleano.values]\n",
    "\n",
    "        # Si la serie temporal no tiene ningún split o contra split no hay nada que hacer.\n",
    "        if len(rentabilidades_excesivas > 0):\n",
    "            multiplicador = pd.DataFrame(np.ones((len(Adj_Close), 1), dtype=int))\n",
    "            print('Para el activo ',activo,' se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica')\n",
    "            print('Se muestran registros afectados: ',rentabilidades_excesivas)\n",
    "\n",
    "            # Recorremos las rentabilidades diarias para construir los multiplicadores.\n",
    "            for dia in range(1, len(Adj_Close.columns) + 1):\n",
    "                if rentabilidades.iloc[dia-1, :][[activo]][0] > 0.69:  # Han hecho un contra split. Ajustamos el multiplicador.\n",
    "                    multiplicador.iloc[dia, 0] = multiplicador.iloc[dia-1, 0] * (Adj_Close.iloc[dia-1, :][[activo]] / Adj_Close.iloc[dia, :][[activo]])[0]\n",
    "                elif rentabilidades.iloc[dia-1, :][[activo]][0] < -0.69:\n",
    "                    multiplicador.iloc[dia, 0] = multiplicador.iloc[dia-1, 0] * (Adj_Close.iloc[dia-1, :][[activo]] / Adj_Close.iloc[dia, :][[activo]])[0]\n",
    "                else:\n",
    "                    multiplicador.iloc[dia, 0] = multiplicador.iloc[dia-1, 0]\n",
    "\n",
    "            # Ajustamos los datos utilizando los multiplicadores.\n",
    "            print('Se ha procedido al ajuste de la serie')\n",
    "\n",
    "            Adj_Close[[activo]] = Adj_Close[[activo]] * multiplicador.values\n",
    "            High[[activo]] = High[[activo]] * multiplicador.values\n",
    "            Low[[activo]] = Low[[activo]] * multiplicador.values\n",
    "            Open[[activo]] = Open[[activo]] * multiplicador.values\n",
    "\n",
    "        else:\n",
    "            print('Para el activo',activo,'no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica')\n",
    "            \n",
    "    return Adj_Close, High, Low, Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_OCHLV_fichero(Open, Adj_Close, High, Low, Volume, tipo, ExcelFile = 'Series_Ajustadas'):\n",
    "    \"\"\"Esta funcion salva los df asociados a .csv\n",
    "    los DF: Open, Adj_Close, High, Low, Volume\n",
    "    tipo:\n",
    "    csv: genera 5 ficheros csv\n",
    "    Excel: genera un libro excel con 5 pestanas\"\"\"\n",
    "    \n",
    "    if tipo == 'Excel':\n",
    "        with pd.ExcelWriter(ExcelFile+'.xlsx') as writer:  \n",
    "            Open.to_excel(writer, sheet_name=\"Open\",index=True)\n",
    "            Adj_Close.to_excel(writer, sheet_name=\"Adj_Close\",index=True)\n",
    "            High.to_excel(writer, sheet_name=\"High\",index=True)\n",
    "            Low.to_excel(writer, sheet_name=\"Low\",index=True)\n",
    "            Volume.to_excel(writer, sheet_name=\"Volume\",index=True)\n",
    "    else:\n",
    "        Open.to_csv('Open.csv')\n",
    "        Adj_Close.to_csv('Adj_Close.csv')\n",
    "        High.to_csv('High.csv')\n",
    "        Low.to_csv('Low.csv')\n",
    "        Volume.to_csv('Volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoritmo_lite(tickers_de_indices, ventana=60):\n",
    "    \n",
    "    dfacum = componentes_indice_ind_download_2(tickers_de_indices)\n",
    "    fecha_fin=set_dates('end',ventana)\n",
    "    fecha_inicial=set_dates('start',ventana)\n",
    "    print('la fecha de inicio es',fecha_inicial,' y la de fin es',fecha_fin)\n",
    "    dates_df = vector_lab_dates(fecha_inicial, ventana)\n",
    "    dates_df_temp = dates_df\n",
    "\n",
    "    dfacum = historic_prices_series(dfacum['Activo'],\n",
    "                                            '1d', \n",
    "                                            fecha_inicial, \n",
    "                                            fecha_fin, \n",
    "                                            dfacum['Indice'])\n",
    "    \n",
    "    dfacum = depura_df_precios(dfacum)\n",
    "    split_df_acum_by_stock(dfacum)\n",
    "    activos_a_importar = dfacum.stock.unique()\n",
    "    \n",
    "    for activo in activos_a_importar:\n",
    "        globals()[activo] = dfacum[dfacum['stock']==activo]\n",
    "    print('se comienza la depuracion de datos')    \n",
    "    \n",
    "    limpieza_de_datos(dfacum, dates_df_temp)    \n",
    "    print('depuracion de datos finalizada')\n",
    "    \n",
    "    OCHLV_DF = genera_OCHLV_DF(dfacum, dates_df)\n",
    "    Open = OCHLV_DF[0]\n",
    "    High = OCHLV_DF[1]\n",
    "    Low = OCHLV_DF[2]\n",
    "    Adj_Close = OCHLV_DF[3]\n",
    "    Volume = OCHLV_DF[4]\n",
    "    print('construccion de DataFrames de Adj_Close, High, Low, Open finalizada')\n",
    "\n",
    "    rentabilidades = rentabilidades_diarias(Adj_Close)\n",
    "    \n",
    "    ajusta_serie_split_results = ajusta_serie_split(rentabilidades, Adj_Close, High, Low, Open)\n",
    "\n",
    "    Adj_Close = ajusta_serie_split_results[0]\n",
    "    High = ajusta_serie_split_results[1]\n",
    "    Low = ajusta_serie_split_results[2]\n",
    "    Open = ajusta_serie_split_results[3]\n",
    "    \n",
    "    print('Ajustes de series por split y contra split de de DataFrames de Adj_Close, High, Low, Open finalizada')\n",
    "    save_OCHLV_fichero(Open, Adj_Close, High, Low, Volume, 'Excel') # Guardamos los DF finales en Excel.\n",
    "    save_OCHLV_fichero(Open, Adj_Close, High, Low, Volume, 'csv') # Guardamos los DF finales en csv.\n",
    "    \n",
    "    print('Ficheros generados correctamente')   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_de_indices = (\"%5EBFX\",\"%5EIBEX\",\"%5EDJI\")                    \n",
    "                 \n",
    "ventana = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%5EBFX EUR\n",
      "El índice  %5EBFX  se ha descargado correctamente\n",
      "%5EIBEX EUR\n",
      "El índice  %5EIBEX  se ha descargado correctamente\n",
      "%5EDJI USD\n",
      "El índice  %5EDJI  se ha descargado correctamente\n",
      "la fecha de inicio es 2019-02-27  y la de fin es 2020-07-11\n",
      "descargando UNH del indice %5EDJI\n",
      "descargando MRK del indice %5EDJI\n",
      "descargando JNJ del indice %5EDJI\n",
      "descargando CSCO del indice %5EDJI\n",
      "descargando V del indice %5EDJI\n",
      "descargando AAPL del indice %5EDJI\n",
      "descargando MCD del indice %5EDJI\n",
      "descargando MSFT del indice %5EDJI\n",
      "descargando HD del indice %5EDJI\n",
      "descargando MMM del indice %5EDJI\n",
      "descargando VZ del indice %5EDJI\n",
      "descargando NKE del indice %5EDJI\n",
      "descargando PFE del indice %5EDJI\n",
      "descargando PG del indice %5EDJI\n",
      "descargando CAT del indice %5EDJI\n",
      "descargando INTC del indice %5EDJI\n",
      "descargando DIS del indice %5EDJI\n",
      "descargando IBM del indice %5EDJI\n",
      "descargando WMT del indice %5EDJI\n",
      "descargando RTX del indice %5EDJI\n",
      "descargando KO del indice %5EDJI\n",
      "descargando WBA del indice %5EDJI\n",
      "descargando AXP del indice %5EDJI\n",
      "descargando BA del indice %5EDJI\n",
      "descargando CVX del indice %5EDJI\n",
      "descargando XOM del indice %5EDJI\n",
      "descargando DOW del indice %5EDJI\n",
      "descargando TRV del indice %5EDJI\n",
      "descargando GS del indice %5EDJI\n",
      "descargando JPM del indice %5EDJI\n",
      "descargando AENA.MC del indice %5EIBEX\n",
      "descargando REE.MC del indice %5EIBEX\n",
      "descargando VIS.MC del indice %5EIBEX\n",
      "descargando ITX.MC del indice %5EIBEX\n",
      "descargando ANA.MC del indice %5EIBEX\n",
      "descargando FER.MC del indice %5EIBEX\n",
      "descargando MRL.MC del indice %5EIBEX\n",
      "descargando TL5.MC del indice %5EIBEX\n",
      "descargando NTGY.MC del indice %5EIBEX\n",
      "descargando CLNX.MC del indice %5EIBEX\n",
      "descargando MEL.MC del indice %5EIBEX\n",
      "descargando ACX.MC del indice %5EIBEX\n",
      "descargando GRF.MC del indice %5EIBEX\n",
      "descargando SGRE.MC del indice %5EIBEX\n",
      "descargando IBE.MC del indice %5EIBEX\n",
      "descargando TEF.MC del indice %5EIBEX\n",
      "descargando AMS.MC del indice %5EIBEX\n",
      "descargando ELE.MC del indice %5EIBEX\n",
      "descargando COL.MC del indice %5EIBEX\n",
      "descargando SAN.MC del indice %5EIBEX\n",
      "descargando IAG.MC del indice %5EIBEX\n",
      "descargando ENG.MC del indice %5EIBEX\n",
      "descargando MAP.MC del indice %5EIBEX\n",
      "descargando CABK.MC del indice %5EIBEX\n",
      "descargando BBVA.MC del indice %5EIBEX\n",
      "descargando MTS.MC del indice %5EIBEX\n",
      "descargando ENC.MC del indice %5EIBEX\n",
      "descargando BKT.MC del indice %5EIBEX\n",
      "descargando ACS.MC del indice %5EIBEX\n",
      "descargando SAB.MC del indice %5EIBEX\n",
      "descargando UCB.BR del indice %5EBFX\n",
      "descargando KBC.BR del indice %5EBFX\n",
      "descargando COFB.BR del indice %5EBFX\n",
      "descargando ONTEX.BR del indice %5EBFX\n",
      "se comienza la depuracion de datos\n",
      "depuracion de datos finalizada\n",
      "construccion de DataFrames de Adj_Close, High, Low, Open finalizada\n",
      "Para el activo UNH no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo MRK no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo JNJ no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo CSCO no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo V no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo AAPL no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo MCD no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo MSFT no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo HD no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo MMM no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo VZ no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo NKE no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo PFE no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo PG no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo CAT no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo INTC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo DIS no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo IBM no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo WMT no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo RTX no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo KO no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo WBA no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo AXP no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo BA no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo CVX no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo XOM no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo DOW no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo TRV no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo GS no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo JPM no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo AENA.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo REE.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo VIS.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo ITX.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo ANA.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo FER.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo MRL.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo TL5.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo NTGY.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo CLNX.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo MEL.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo ACX.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo GRF.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo SGRE.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo IBE.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo TEF.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo AMS.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo ELE.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo COL.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo SAN.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo IAG.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo ENG.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo MAP.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo CABK.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo BBVA.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo MTS.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo ENC.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo BKT.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo ACS.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo SAB.MC no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo UCB.BR no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo KBC.BR no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo COFB.BR no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Para el activo ONTEX.BR no se han identificado Splits no ajustados bajo el enfoque de saltos del +-69% rentabilidad logaritmica\n",
      "Ajustes de series por split y contra split de de DataFrames de Adj_Close, High, Low, Open finalizada\n",
      "Ficheros generados correctamente\n"
     ]
    }
   ],
   "source": [
    "algoritmo_lite(tickers_de_indices, ventana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para poder debugear,  dejo las funciones, paso a paso y explicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%5EBFX EUR\n",
      "El índice  %5EBFX  se ha descargado correctamente\n",
      "%5EIBEX EUR\n",
      "El índice  %5EIBEX  se ha descargado correctamente\n",
      "%5EDJI USD\n",
      "El índice  %5EDJI  se ha descargado correctamente\n"
     ]
    }
   ],
   "source": [
    "dfacum = componentes_indice_ind_download_2(tickers_de_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la fecha de inicio es 2019-02-21  y la de fin es 2020-07-05\n"
     ]
    }
   ],
   "source": [
    "fecha_fin=set_dates('end',ventana)\n",
    "fecha_inicial=set_dates('start',ventana)\n",
    "\n",
    "print('la fecha de inicio es',fecha_inicial,' y la de fin es',fecha_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "incluimos el tratamento de fechas del enunciado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df = vector_lab_dates(fecha_inicial, ventana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descargando AAPL del indice %5EDJI\n",
      "descargando TRV del indice %5EDJI\n",
      "descargando GS del indice %5EDJI\n",
      "descargando KO del indice %5EDJI\n",
      "descargando HD del indice %5EDJI\n",
      "descargando CSCO del indice %5EDJI\n",
      "descargando UNH del indice %5EDJI\n",
      "descargando VZ del indice %5EDJI\n",
      "descargando BA del indice %5EDJI\n",
      "descargando RTX del indice %5EDJI\n"
     ]
    }
   ],
   "source": [
    "dfacum = historic_prices_series(dfacum['Activo'],\n",
    "                                '1d', \n",
    "                                fecha_inicial, \n",
    "                                fecha_fin, \n",
    "                                dfacum['Indice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algunas veces se descargan datos pero la info no es buena, generandose registros y columnas de suciedad\n",
    "\n",
    "def depura_df_precios(dfacum):\n",
    "    dfacum=dfacum.loc[:,['Adj Close','Close', 'Date','High','Low','Open','Volume','indice','stock']]\n",
    "    (dfacum[(dfacum.Date).isna()]) = (dfacum[(dfacum.Date).isna()]).dropna()\n",
    "    return dfacum    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfacum = depura_df_precios(dfacum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depuramos del DF de Suciedad\n",
    "\n",
    "dfacum=dfacum.loc[:,['Adj Close','Close', 'Date','High','Low','Open','Volume','indice','stock']]\n",
    "(dfacum[(dfacum.Date).isna()]) = (dfacum[(dfacum.Date).isna()]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df_acum_by_stock(dfacum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activos_a_importar = dfacum.stock.unique()\n",
    "\n",
    "for activo in activos_a_importar:\n",
    "    globals()[activo] = dfacum[dfacum['stock']==activo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volcamos los datos en un DF homogeneizado, utilizando la columna de fechas como índice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completamos los datos con NA (días en los que no hay registrada una cotización, pero que otras empresas sí cotizaron) na.locf localiza los NA del DF y los sustituye por el valor de la fila anterior. Si la fila con NA es la 1ª deja el NA sin dar un error.\n",
    "\n",
    "utilizo esta solucion\n",
    "  \n",
    "  http://pyciencia.blogspot.com/2015/04/trabajar-con-datos-nan-en-dataframe.html\n",
    "  \n",
    "_Con los valores vecinos\n",
    "Reemplazar los NaN con el valor anterior o posterior del NaN: 'pad' para reemplazarlo con el valor anterior y 'bfill' con el posterior. Se reemplazan todos los NaN, pero se puede establecer un límite según la distancia de este con el último dato del dataframe. Esta distancia se especifica en limit:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activos_a_importar = dfacum.stock.unique()\n",
    "\n",
    "for activo in activos_a_importar:\n",
    "    globals()[activo] = dataframe_date_adj(globals()[activo], dates_df)\n",
    "    globals()[activo] = remplaza_nulos_na(globals()[activo], globals()[activo].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCHLV_DF = genera_OCHLV_DF(dfacum, dates_df)\n",
    "Open = OCHLV_DF[0]\n",
    "High = OCHLV_DF[1]\n",
    "Low = OCHLV_DF[2]\n",
    "Adj_Close = OCHLV_DF[3]\n",
    "Volume = OCHLV_DF[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activos_a_importar = dfacum.stock.unique()\n",
    "fechas=len(dates_df.index)\n",
    "nactivos=len(activos_a_importar)\n",
    "matriz=np.zeros((fechas, nactivos))\n",
    "carcasa=pd.DataFrame(matriz, index=dates_df.index, columns=activos_a_importar)\n",
    "Open = carcasa.copy()\n",
    "High = carcasa.copy()\n",
    "Low = carcasa.copy()\n",
    "Volume = carcasa.copy()\n",
    "Adj_Close = carcasa.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean los DF de metricas, en donde los activos son encabezados y las fechas filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activo in activos_a_importar:\n",
    "    \n",
    "    datos = globals().get(activo)\n",
    "    Open[activo] = datos.loc[(datos['stock']==activo) == True , 'Open']\n",
    "    High[activo] = datos.loc[(datos['stock']==activo) == True , 'High']\n",
    "    Low[activo] = datos.loc[(datos['stock']==activo) == True , 'Low']\n",
    "    Volume[activo] = datos.loc[(datos['stock']==activo) == True , 'Volume']\n",
    "    Adj_Close[activo] = datos.loc[(datos['stock']==activo) == True , 'Adj Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al asignar los activos a las fechas en base al indice, hemos ido muy rapido, \n",
    "pero se han podido generar algunos huecos adicionales al integrarlos en el DF.\n",
    "En el ejercicio de R esto se hacia del tiron en el ejercicio anterior, pero en Python,\n",
    "lo solvento trabajando con dataframes de menor tamaño hasta este punto, cruzar por indice,\n",
    "y pasar de nuevo por la funcion de nas y nulos. \n",
    "\n",
    "No podemos hacer un dropna, ya que puede haber algun mercado abierto, por lo que pasamos la funcion de relleno de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Open = remplaza_nulos_na(Open, Open.columns)\n",
    "High = remplaza_nulos_na(High, High.columns)\n",
    "Low = remplaza_nulos_na(Low, Low.columns)\n",
    "Volume = remplaza_nulos_na(Volume, Volume.columns)\n",
    "Adj_Close = remplaza_nulos_na(Adj_Close, Adj_Close.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los datos de cierre ajustados no están bien desmanipulados.\n",
    "# calculamos la rentabilidad de los activos\n",
    "\n",
    "rentabilidades = pd.DataFrame(0,\n",
    "                              index=Adj_Close.index,\n",
    "                              columns=Adj_Close.columns)\n",
    "\n",
    "# Ojo, los activos que no nos hemos podido descargar tienen precio de 0 €. La rentabilidad 0/0 dará un error que tenemos que evitar.\n",
    "rentabilidades = np.log(Adj_Close.astype(float)).diff()\n",
    "rentabilidades = rentabilidades.dropna() #eliminamos la tipca primera fila de NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "por fin, llegamos a la parte final del ejercicio:\n",
    "\n",
    " Buscamos split y contrasplit, de existir, hay que \"desmanipular\" la serie de precios para hacerlos homogéneos entre sí.\n",
    "    # Si la rent log es superior al 0.69 el precio se ha duplicado en un día, probablemente han realizado un contrasplit de la acción.\n",
    "    # Si la rent log es inferior al -0.69 el precio se ha dividido entre do en un día, probablemente han realizado un split de la acción.\n",
    "\n",
    "    #             Precio  Rentabilidad  Multiplicador       Precio desmanipulado\n",
    "    # 25/8/16    100        -2.302        1                   100\n",
    "    # 24/8/16   1000         2.302        100/1000            100\n",
    "    # 23/8/16    100                      100/1000*1000/100   100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_Close = Adj_Close.astype(float)\n",
    "High = High.astype(float)\n",
    "Low = Low.astype(float)\n",
    "Open = Open.astype(float)\n",
    "\n",
    "ajusta_serie_split_results = ajusta_serie_split(rentabilidades, Adj_Close, High, Low, Open)\n",
    "\n",
    "Adj_Close = ajusta_serie_split_results[0]\n",
    "High = ajusta_serie_split_results[1]\n",
    "Low = ajusta_serie_split_results[2]\n",
    "Open = ajusta_serie_split_results[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_OCHLV_fichero(Open, Adj_Close, High, Low, Volume, 'Excel', 'Series_Ajustadas2') # Guardamos los DF finales en Excel.\n",
    "save_OCHLV_fichero(Open, Adj_Close, High, Low, Volume, 'csv') # Guardamos los DF finales en csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otra forma de hacerlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_de_indices = [\n",
    "    \"%5EBFX\", \"%5EBVSP\", \"%5EDJI\", \"%5EFCHI\", \"%5EFTSE\",\n",
    "    \"%5EGDAXI\", \"%5EHSI\", \"%5EIBEX\", \"%5EMXX\", \"%5EJKSE\",\n",
    "    \"%5EMERV\", \"%5EOMXSPI\", \"%5EOSEAX\", \"%5ESSMI\", \"%5ESTI\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventana = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homogeneizar(datos, ventana):\n",
    "\n",
    "    # Fechas inicio y fin\n",
    "    fecha_inicial = (datetime.now() - relativedelta(days=ventana)).date()\n",
    "    fecha_fin = datetime.now().date()\n",
    "\n",
    "    # Creamos un vector con todas las fechas entre el día inicial y el final, sin fines de semana\n",
    "    fechas = pd.date_range(start=fecha_inicial, end=fecha_fin, freq='B')\n",
    "\n",
    "    # Creamos dataframe con la columna de fechas\n",
    "    fechas = fechas.to_frame(index=False)\n",
    "    fechas.columns = ['Date']\n",
    "\n",
    "    # Volcamos los datos en un DF homogeneizado\n",
    "    datos_homogeneizados = pd.merge(datos, fechas, how='outer', on='Date')\n",
    "\n",
    "    # Manetemos orden descedente\n",
    "    datos_homogeneizados.sort_values(by='Date', ascending=False, inplace=True)\n",
    "\n",
    "    # Sustituir NA por el valor de la fila anterior. Si la fila con NA es la 1ª deja el NA sin dar un error.\n",
    "    datos_homogeneizados.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Si los NA están en las primeras filas no hemos solucionado el problema. En principio no debería de haber más que dos (sábado y domingo), \n",
    "    # pero una empresa podría no cotizar lunes, martes... por lo que no conocemos el nº de potenciales NA a resolver.\n",
    "    # Para resolver el problema, invertimos el orden del DF, aplicamos na.locf de nuevo y devolvemos el DF a su posición original.\n",
    "    datos_homogeneizados.sort_values(by='Date', ascending=True, inplace=True)\n",
    "    datos_homogeneizados.fillna(method='ffill', inplace=True)\n",
    "    datos_homogeneizados.sort_values(by='Date', ascending=False, inplace=True)\n",
    "\n",
    "    return datos_homogeneizados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos la función que intenta obtener los datos del índice.\n",
    "def obtener_indice(url, cookies):\n",
    "\n",
    "    try:\n",
    "        benchmark = requests.get(url, cookies=cookies).text\n",
    "        benchmark = io.StringIO(benchmark)\n",
    "    except Exception as e:\n",
    "        print(\"No nos hemos podido descargar el indice\")\n",
    "        print(\"Este es el mensaje de error que ha dado:\")\n",
    "        print(e)\n",
    "\n",
    "        return(\"Sin datos\")\n",
    "\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que intenta obtener los datos de cada activo\n",
    "def obtener_datos_activo(activo, fecha_inicial, fecha_fin, columna_fechas):\n",
    "\n",
    "    info_activo = pd.DataFrame()  # Inicializamos variable a dataframe vacío\n",
    "\n",
    "    try:\n",
    "\n",
    "        # html de la web del activo\n",
    "        request = HTMLSession().get(f\"https://es.finance.yahoo.com/quote/{activo}?ltr=1\")\n",
    "        res = request.text\n",
    "\n",
    "        # Obtenemos el CrumbStore\n",
    "        pattern_crumbstore = r'\\\"CrumbStore\\\"\\:{\\\"crumb\\\":\\\"(?P<crumb>.*?)\\\"}'\n",
    "        crumb = re.search(pattern_crumbstore, res).groupdict().get('crumb')\n",
    "        crumb = crumb.replace(\"\\n\", \"\")  # Algunas veces, el crumb tiene una función de escape (un salto de página).\n",
    "\n",
    "        # Construimos la url para bajarnos los datos\n",
    "        url = f\"https://query1.finance.yahoo.com/v7/finance/download/{activo}?period1={fecha_inicial}&period2={fecha_fin}&interval=1d&events=history&crumb={crumb}\"\n",
    "\n",
    "        # info activo dataframe\n",
    "        html_text = HTMLSession().get(url, cookies=request.cookies).text\n",
    "        info_activo = pd.read_csv(io.StringIO(html_text))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    finally:\n",
    "\n",
    "        if len(info_activo) == 0:\n",
    "            print(f\"No nos hemos podido descargar el activo {activo}\")\n",
    "\n",
    "            # Generamos un DF a ceros, con el tamaño adecuado, para que los siguientes pasos no den error.\n",
    "            info_activo = pd.DataFrame(0,\n",
    "                                       index=columna_fechas,\n",
    "                                       columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"])\n",
    "            info_activo.index.name = 'Date'\n",
    "            info_activo = info_activo.reset_index()  # Respetamos la misma estructura que el CSV que obtenemos de yahoo sin indice\n",
    "\n",
    "        return info_activo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoritmo_lite(indice, ventana=60):\n",
    "\n",
    "    print(indice)\n",
    "\n",
    "    # Extraemos el índice del listado de índices\n",
    "    benchmark = indice\n",
    "\n",
    "    # Establecemos el rango de fechas que queremos importar.\n",
    "    fecha_inicial = (datetime.now() - relativedelta(days=ventana)).strftime('%s')\n",
    "    fecha_fin = datetime.now().strftime('%s')\n",
    "\n",
    "    # html de la web del benchmark\n",
    "    request = HTMLSession().get(f\"https://es.finance.yahoo.com/quote/{benchmark}?ltr=1\")\n",
    "    res = request.text\n",
    "\n",
    "    # Obtenemos el CrumbStore\n",
    "    pattern_crumbstore = r'\\\"CrumbStore\\\"\\:{\\\"crumb\\\":\\\"(?P<crumb>.*?)\\\"}'\n",
    "    crumb = re.search(pattern_crumbstore, res).groupdict().get('crumb')\n",
    "    crumb = crumb.replace(\"\\n\", \"\")  # Algunas veces, el crumb tiene una función de escape (un salto de página).\n",
    "\n",
    "    # Construimos la url para bajarnos los datos\n",
    "    url = f\"https://query1.finance.yahoo.com/v7/finance/download/{benchmark}?period1={fecha_inicial}&period2={fecha_fin}&interval=1d&events=history&crumb={crumb}\"\n",
    "\n",
    "    # Obtenemos benckmark\n",
    "    intentos = 0\n",
    "    benchmark = 'Sin datos'\n",
    "    while benchmark == \"Sin datos\":\n",
    "\n",
    "        benchmark = obtener_indice(url, request.cookies)\n",
    "\n",
    "        intentos += 1\n",
    "        if intentos > 3:\n",
    "            benchmark == \"No descargable\"\n",
    "            print(\"Tras varios intentos no nos hemos podido descargar los fondos. Dejamos de intentarlo.\")\n",
    "            return None\n",
    "\n",
    "        if benchmark == \"Sin datos\":\n",
    "            print(\"Error en la descarga de los fondos, reintentamos\")\n",
    "            time.sleep(30)\n",
    "\n",
    "    benchmark = pd.read_csv(benchmark)\n",
    "\n",
    "    # Hay veces que el último día se descarga dos veces en el DF. La segunda vez, la columna de volumen está a cero.\n",
    "    if benchmark.iloc[-1, ]['Date'] == benchmark.iloc[-2, ]['Date']:\n",
    "        benchmark = benchmark[:-1]\n",
    "\n",
    "    # Cogemos la columna de fechas del índice para usarla cuando no nos consigamos bajar algún activo.\n",
    "    columna_fechas = benchmark['Date']\n",
    "\n",
    "    # Obtenemos los activos que contiene cada índice.\n",
    "    intentos = 0\n",
    "    descarga = \"Sin datos\"\n",
    "\n",
    "    while descarga == \"Sin datos\":\n",
    "\n",
    "        try:\n",
    "            datos_web = HTMLSession().get(f\"https://es.finance.yahoo.com/quote/{indice}/components/\").html\n",
    "            descarga = 'OK'\n",
    "        except Exception:\n",
    "            intentos = intentos + 1\n",
    "\n",
    "            if intentos > 3:\n",
    "                print(\"Tras varios intentos no nos hemos podido descargar los activos del índice. Dejamos de intentarlo.\")\n",
    "                return None\n",
    "\n",
    "            if descarga == \"Sin datos\":\n",
    "                print(\"Error en la descarga de los activos del índice, reintentamos\")\n",
    "                time.sleep(30)\n",
    "\n",
    "    # activos del indice\n",
    "    activos = pd.read_html(datos_web.find('table', first=True).html)[0]\n",
    "\n",
    "    if len(activos) >= 1:  # Comprobamos si Yahoo publica algún activo del índice\n",
    "\n",
    "        # Ojo, queremos importar únicamente los activos que tengan cotización.\n",
    "        activos_con_datos = activos.dropna(subset=['Último precio'])\n",
    "\n",
    "        # simbolo de cada empresa del índice\n",
    "        activos_a_importar = activos_con_datos['Símbolo'].to_list()\n",
    "\n",
    "        # nombre de cada empresa del índice\n",
    "        nombres_empresas = activos_con_datos['Nombre de la empresa'].to_list()\n",
    "\n",
    "        # divisa de las empresas del índice\n",
    "        divisa_empresas = (datos_web\n",
    "                           .find('#Col1-0-Components-Proxy', first=True)\n",
    "                           .find('span', first=True)\n",
    "                           .text\n",
    "                           .split(' ')[-1])\n",
    "    else:\n",
    "        print(f\"El índice {indice} no tiene activos descargables\")\n",
    "\n",
    "\n",
    "    # Importamos los activos que hemos seleccionado.\n",
    "    for activo in activos_a_importar:\n",
    "\n",
    "        intentos = 0\n",
    "        descarga = \"Sin datos\"\n",
    "\n",
    "        while descarga == \"Sin datos\":\n",
    "\n",
    "            info_activo = obtener_datos_activo(activo, fecha_inicial, fecha_fin, columna_fechas)\n",
    "\n",
    "            # Comprobamos que nos hemos podido descargar el activo.\n",
    "            if len(info_activo) > 0:\n",
    "                descarga = 'OK'\n",
    "\n",
    "            intentos = intentos + 1\n",
    "\n",
    "            if intentos > 3:\n",
    "                print(f\"Tras varios intentos no nos hemos podido descargar {activo} Dejamos de intentarlo.\")\n",
    "                descarga = 'OK'\n",
    "\n",
    "            if descarga == \"Sin datos\":\n",
    "                print(f\"Error en la descarga del activo {activo} reintentamos\")\n",
    "                time.sleep(30)\n",
    "\n",
    "        globals()[activo] = info_activo\n",
    "\n",
    "    # Homogeneizamos los datos (que todos los DF tengan el mismo número de filas / días y que estén puestos en las mismas líneas)\n",
    "    for activo in activos_a_importar:\n",
    "\n",
    "        datos = globals().get(activo)\n",
    "\n",
    "        # Comprobamos si nos hemos podido descargar el activo, o si nos hemos descargado más de un único día.\n",
    "        if len(datos) >= 1:\n",
    "\n",
    "            # Hay veces que el último día se descarga dos veces en el DF. La segunda vez, la columna de volumen está a cero.\n",
    "            if datos.iloc[-1, ]['Date'] == datos.iloc[-2, ]['Date']:\n",
    "                datos = datos[:-1]\n",
    "\n",
    "            # Hay veces que hay datos duplicados entre medias del vector. Eliminamos las filas duplicadas.\n",
    "            datos.drop_duplicates(subset=\"Date\")\n",
    "\n",
    "        # Confirmamos tipo date para la fecha\n",
    "        datos['Date'] = pd.to_datetime(datos['Date'])\n",
    "        datos_homogeneizados = homogeneizar(datos, ventana)\n",
    "\n",
    "        # Guardamos el DF homogeneizado en su variable original.\n",
    "        globals()[activo] = datos_homogeneizados\n",
    "\n",
    "    # Creamos los DF donde guardaremos los resultados (excel puede contener algo más de 16.000 columnas)\n",
    "    Open = pd.DataFrame(0, index=datos_homogeneizados['Date'], columns=activos_a_importar)\n",
    "    High = pd.DataFrame(0, index=datos_homogeneizados['Date'], columns=activos_a_importar)\n",
    "    Low = pd.DataFrame(0, index=datos_homogeneizados['Date'], columns=activos_a_importar)\n",
    "    Volume = pd.DataFrame(0, index=datos_homogeneizados['Date'], columns=activos_a_importar)\n",
    "    Adj_close = pd.DataFrame(0, index=datos_homogeneizados['Date'], columns=activos_a_importar)\n",
    "\n",
    "    # Guardamos los datos en los DF finales\n",
    "    for activo in activos_a_importar:\n",
    "\n",
    "        datos = globals().get(activo)\n",
    "\n",
    "        Open[activo] = datos['Open'].values\n",
    "        High[activo] = datos['High'].values\n",
    "        Low[activo] = datos['Low'].values\n",
    "        Adj_close[activo] = datos['Adj Close'].values\n",
    "        Volume[activo] = datos['Volume'].values\n",
    "\n",
    "    # Los datos de cierre ajustados no están bien desmanipulados.\n",
    "    # calculamos la rentabilidad de los activos\n",
    "    rentabilidades = pd.DataFrame(0,\n",
    "                                  index=Adj_close.index,\n",
    "                                  columns=Adj_close.columns)\n",
    "\n",
    "    # Ojo, los activos que no nos hemos podido descargar tienen precio de 0 €. La rentabilidad 0/0 dará un error que tenemos que evitar.\n",
    "    rentabilidades = np.log(Adj_close).diff().shift(-1) * -1\n",
    "    rentabilidades.iloc[-1, ] = rentabilidades.iloc[-2, ]\n",
    "\n",
    "    # Aquellos que fueran todo 0s, tendremos NaN al hacer el np.log\n",
    "    # Ponemos 0s en los NaN\n",
    "    rentabilidades.fillna(0, inplace=True)\n",
    "\n",
    "    # Buscamos split y contrasplit, de existir, hay que \"desmanipular\" la serie de precios para hacerlos homogéneos entre sí.\n",
    "    # Si la rent log es superior al 0.69 el precio se ha duplicado en un día, probablemente han realizado un contrasplit de la acción.\n",
    "    # Si la rent log es inferior al -0.69 el precio se ha dividido entre do en un día, probablemente han realizado un split de la acción.\n",
    "\n",
    "    #             Precio  Rentabilidad  Multiplicador       Precio desmanipulado\n",
    "    # 25/8/16    100        -2.302        1                   100\n",
    "    # 24/8/16   1000         2.302        100/1000            100\n",
    "    # 23/8/16    100                      100/1000*1000/100   100\n",
    "\n",
    "    for activo in rentabilidades.columns:\n",
    "\n",
    "        # Comprobamos si alguna rentabilidad de la serie es mayor o menor a 0.69 y calculamos los multiplicadores.\n",
    "        indice_booleano = (rentabilidades[[activo]] > 0.69) | (rentabilidades[[activo]] < -0.69)\n",
    "        rentabilidades_excesivas = rentabilidades[[activo]].loc[indice_booleano.values]\n",
    "\n",
    "        # Si la serie temporal no tiene ningún split o contra split no hay nada que hacer.\n",
    "        if len(rentabilidades_excesivas > 0):\n",
    "            multiplicador = pd.DataFrame(np.ones((len(Adj_close), 1), dtype=int))\n",
    "\n",
    "            # Recorremos las rentabilidades diarias para construir los multiplicadores.\n",
    "            for dia in range(1, len(Adj_close.columns) + 1):\n",
    "                if rentabilidades.iloc[dia-1, :][[activo]][0] > 0.69:  # Han hecho un contra split. Ajustamos el multiplicador.\n",
    "                    multiplicador.iloc[dia, 0] = multiplicador.iloc[dia-1, 0] * (Adj_close.iloc[dia-1, :][[activo]] / Adj_close.iloc[dia, :][[activo]])[0]\n",
    "                elif rentabilidades.iloc[dia-1, :][[activo]][0] < -0.69:\n",
    "                    multiplicador.iloc[dia, 0] = multiplicador.iloc[dia-1, 0] * (Adj_close.iloc[dia-1, :][[activo]] / Adj_close.iloc[dia, :][[activo]])[0]\n",
    "                else:\n",
    "                    multiplicador.iloc[dia, 0] = multiplicador.iloc[dia-1, 0]\n",
    "\n",
    "            # Ajustamos los datos utilizando los multiplicadores.\n",
    "            Adj_close[[activo]] = Adj_close[[activo]] * multiplicador.values\n",
    "            High[[activo]] = High[[activo]] * multiplicador.values\n",
    "            Low[[activo]] = Low[[activo]] * multiplicador.values\n",
    "            Open[[activo]] = Open[[activo]] * multiplicador.values\n",
    "\n",
    "    # Guardamos los DF finales en csv.\n",
    "    High.to_csv('High.csv')\n",
    "    Low.to_csv('Low.csv')\n",
    "    Volume.to_csv('Volume.csv')\n",
    "    Adj_close.to_csv('Adj_close.csv')\n",
    "    Open.to_csv('Open.csv')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indice in tickers_de_indices:\n",
    "    \n",
    "    # Invocamos a la función pasándola los índices uno a uno.\n",
    "    recomendacion = algoritmo_lite(indice, ventana=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('venv-ml': venv)",
   "language": "python",
   "name": "python36864bitvenvmlvenve40bc169c9714114b4130d652b923ecf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
