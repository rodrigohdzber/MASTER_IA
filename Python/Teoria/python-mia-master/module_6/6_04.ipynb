{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4. OptimizaciÃ³n de carteras II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import cvxopt as opt\n",
    "from cvxopt import blas, solvers\n",
    "import ffn\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "data_range = '5y'\n",
    "dict_data = {\n",
    "    'AAPL': utils.get_data_iex('AAPL', st_range=data_range).close,\n",
    "    'FB': utils.get_data_iex('FB', st_range=data_range).close,\n",
    "    'JPM': utils.get_data_iex('JPM', st_range=data_range).close,\n",
    "    'BRK.B': utils.get_data_iex('BRK.B', st_range=data_range).close,\n",
    "    'GOOG': utils.get_data_iex('GOOG', st_range=data_range).close,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_close_all_portfolio = pd.DataFrame(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = data_close_all_portfolio.pct_change().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClusterVar(cov,cItems):\n",
    "    # Compute variance per cluster\n",
    "    cov_=cov.loc[cItems,cItems] # matrix slice\n",
    "    w_=getIVP(cov_).reshape(-1,1)\n",
    "    cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "    return cVar\n",
    "\n",
    "\n",
    "def getQuasiDiag(link):\n",
    "    # Sort clustered items by distance\n",
    "    link = link.astype(int)\n",
    "    sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "    numItems = link[-1, 3]  # number of original items\n",
    "    while sortIx.max() >= numItems:\n",
    "        sortIx.index = range(0, sortIx.shape[0] * 2, 2)  # make space\n",
    "        df0 = sortIx[sortIx >= numItems]  # find clusters\n",
    "        i = df0.index\n",
    "        j = df0.values - numItems\n",
    "        sortIx[i] = link[j, 0]  # item 1\n",
    "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
    "        sortIx = sortIx.append(df0)  # item 2\n",
    "        sortIx = sortIx.sort_index()  # re-sort\n",
    "        sortIx.index = range(sortIx.shape[0])  # re-index\n",
    "    return sortIx.tolist()\n",
    "\n",
    "\n",
    "def getRecBipart(cov, sortIx):\n",
    "    # Compute HRP alloc\n",
    "    w = pd.Series(1, index=sortIx)\n",
    "    cItems = [sortIx]  # initialize all items in one cluster\n",
    "    while len(cItems) > 0:\n",
    "        cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]  # bi-section\n",
    "        for i in range(0, len(cItems), 2):  # parse in pairs\n",
    "            cItems0 = cItems[i]  # cluster 1\n",
    "            cItems1 = cItems[i + 1]  # cluster 2\n",
    "            cVar0 = getClusterVar(cov, cItems0)\n",
    "            cVar1 = getClusterVar(cov, cItems1)\n",
    "            alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
    "            w[cItems0] *= alpha  # weight 1\n",
    "            w[cItems1] *= 1 - alpha  # weight 2\n",
    "    return w\n",
    "\n",
    "\n",
    "def correlDist(corr):\n",
    "    # A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
    "    # This is a proper distance metric\n",
    "    # distance matrix\n",
    "    dist = ((1 - corr) / 2.)**.5  \n",
    "    return dist\n",
    "\n",
    "\n",
    "def getHRP(cov, corr):\n",
    "    # Construct a hierarchical portfolio\n",
    "    dist = correlDist(corr)\n",
    "    link = sch.linkage(dist, 'single')\n",
    "    #dn = sch.dendrogram(link, labels=cov.index.values, label_rotation=90)\n",
    "    #plt.show()\n",
    "    sortIx = getQuasiDiag(link)\n",
    "    sortIx = corr.index[sortIx].tolist()\n",
    "    hrp = getRecBipart(cov, sortIx)\n",
    "    return hrp.sort_index()\n",
    "\n",
    "\n",
    "def getIVP(cov, **kargs):\n",
    "    # Compute the inverse-variance portfolio\n",
    "    ivp = 1. / np.diag(cov)\n",
    "    ivp /= ivp.sum()\n",
    "    return ivp\n",
    "\n",
    "def getMVP(cov):\n",
    "    cov = cov.T.values\n",
    "    n = len(cov)\n",
    "    N = 100\n",
    "    mus = [\n",
    "        10 ** (5.0 * t / N - 1.0) \n",
    "        for t in range(N)\n",
    "    ]\n",
    "    # Convert to cvxopt matrices\n",
    "    S = opt.matrix(cov)\n",
    "    #pbar = opt.matrix(np.mean(returns, axis=1))\n",
    "    pbar = opt.matrix(np.ones(cov.shape[0]))\n",
    "    # Create constraint matrices\n",
    "    G = -opt.matrix(np.eye(n))  # negative n x n identity matrix\n",
    "    h = opt.matrix(0.0, (n, 1))\n",
    "    A = opt.matrix(1.0, (1, n))\n",
    "    b = opt.matrix(1.0)\n",
    "    # Calculate efficient frontier weights using quadratic programming\n",
    "    portfolios = [solvers.qp(mu * S, -pbar, G, h, A, b)['x']\n",
    "                  for mu in mus]\n",
    "    ## CALCULATE RISKS AND RETURNS FOR FRONTIER\n",
    "    returns = [blas.dot(pbar, x) for x in portfolios]\n",
    "    risks = [np.sqrt(blas.dot(x, S * x)) for x in portfolios]\n",
    "    ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE\n",
    "    m1 = np.polyfit(returns, risks, 2)\n",
    "    x1 = np.sqrt(m1[2] / m1[0])\n",
    "    # CALCULATE THE OPTIMAL PORTFOLIO\n",
    "    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n",
    "    return list(wt)\n",
    "\n",
    "\n",
    "def get_all_portfolios(returns):  \n",
    "    cov, corr = returns.cov(), returns.corr()\n",
    "    hrp = getHRP(cov, corr)\n",
    "    ivp = getIVP(cov)\n",
    "    ivp = pd.Series(ivp, index=cov.index)\n",
    "    mvp = getMVP(cov)\n",
    "    mvp = pd.Series(mvp, index=cov.index)   \n",
    "    portfolios = pd.DataFrame([mvp, ivp, hrp], index=['MVP', 'IVP', 'HRP']).T  \n",
    "    return portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios = get_all_portfolios(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Form the book: Lopez de prado\n",
    "- Tree clustering: group similar investments into clusters based on their correlation matrix. Having a hierarchical structure helps us to improve stability issues of quadratic optimizers when inverting the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov, corr = returns.cov(), returns.corr()\n",
    "dist = correlDist(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = sch.linkage(dist, 'single')\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "_ = sch.dendrogram(link, labels=returns.columns, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
